{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdkFRx6sFsE8"
   },
   "source": [
    "# myInstructGPT: Reinforcement Learning from Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uedxrz5xFsE_"
   },
   "source": [
    "In this tutorial, our goal is to use RLHF in practice to build a model that can summarize  Reddit posts. To this end we will use a model initialized from the pretrained 124M parameter GPT-2 model and fine tune it step by step following the Intrsuct-GPT learning process. As shown in class, this process includes the following steps:\n",
    "\n",
    "1. Supervised Fine Tuning (SFT)\n",
    "\n",
    "2. Learning a Reward Model\n",
    "\n",
    "3. Policy Optimization with PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first install and import the libraries we are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch datasets regex transformers matplotlib triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer, CN\n",
    "from mingpt.utils import set_seed, lr_schedule, masked_mean, try_auto_cast\n",
    "from mingpt.logger import Logger\n",
    "from mingpt.rewards import RewardModel, ValueModel, calculate_advantage_and_returns\n",
    "\n",
    "from summarize_rlhf.summarize_gpt import SummarizePrompt, policy_loss, value_loss\n",
    "from summarize_rlhf.summarize_sft import SFTSummarize\n",
    "from summarize_rlhf.summarize_reward_model import RewardModelSummarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPDvpeQUFsFA"
   },
   "source": [
    "## Step 1: Supervised Fine Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"./images/SFT_step1.png\"></center>\n",
    "<center width=\"100%\"><small>Step 1: Fine-tune the model with supervised learning</small><br><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqB3neI0FsFA"
   },
   "source": [
    "**What model and data we will use for SFT?** \n",
    "\n",
    "To fine tune our model using supervised learning, we wll first need training data specific to our application (domain specific). For our summarization task, this means that we need a dataset with input prompts that include the post we want to summarize, to which we can append the characters 'TL;DR:', thus asking essentially our model to generate a summary of the post. In addition, for our dataset, we will need true `labels` for our prompts, that can be example summaries written by human experts. For our task we will use the [CarperAI/openai_summarize_tldr](https://huggingface.co/datasets/CarperAI/openai_summarize_tldr?row=0) dataset, that is publicly available in huggingface. As a pre-trained model, we will use the pre-trained GPT-2 model with ~124M parameters, the weights of which we will also download from hugginface.\n",
    "\n",
    "**Why do we need SFT?**\n",
    "\n",
    "With SFT we leverage the general capabilities of our base model, that may have gone through extensive training, but on the same time try to stir them and specialize them for our specific application. In particular, for our task, we take advantage of the GPT-2 model trained to produce coherent and human understandable text, and further train it to specialize it for post summaries. However, in our case, the good news is that we do need to train the model with millions of data, and we can achieve relatively good performance with much fewer data, saving a lot of time and resources. Another advantage is that we do not necessarily need to retrain the entire model (the last layers may be enough).  \n",
    "\n",
    "**What are the challenges with SFT?**\n",
    "\n",
    "Creating a labeled dataset can be quite tedious and costly, especially because it requires human effort to collect the data and produce the labels for the data. In addition, it may be even impossible to collect such a dataset depending on the application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKAWxf5xFsFC"
   },
   "source": [
    "Let's try it out! We will first define the below callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2xWgTgB3FsFE"
   },
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    trainer.logger.log(\"Train\", trainer.iter_num, trainer.loss.item())\n",
    "\n",
    "    if trainer.iter_num % trainer.config.log_every == 0:\n",
    "        # evaluate both the train and test score\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                batch = [x.to(device) for x in batch]\n",
    "                logits, loss = model(*batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        val_loss = total_loss / (i+1)\n",
    "        trainer.logger.log(\"Valid\", trainer.iter_num, val_loss)\n",
    "        print(f\"E: {trainer.epoch}, iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}, val loss: {val_loss:.5f}\")\n",
    "\n",
    "    if trainer.iter_num % trainer.config.generate_every == 0:\n",
    "        with torch.no_grad():\n",
    "            sample_prompt = prompt_ds[17].to(device)\n",
    "            idx = model.generate(sample_prompt, max_new_tokens=128, do_sample=True, top_k=30, stop_at=train_ds.tokenizer.eot_token).cpu()\n",
    "            for j,generation in enumerate(idx):\n",
    "                print(f\"Generation {j}:\", train_ds.tokenizer.decode(generation))\n",
    "\n",
    "    # save the latest model\n",
    "    if trainer.config.save_every and trainer.iter_num % trainer.config.save_every == 0:\n",
    "        print(\"saving model\")\n",
    "        ckpt_path = os.path.join(os.path.curdir, \"model_sft.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "    # revert model to training mode\n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our training data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== STARTING SUPERVISED FINE-TUNING =====\n",
      "Loading Data...\n",
      "Data Loaded\n",
      "Creating Model...\n",
      "number of parameters: 124.44M\n",
      "Model Created\n",
      "Configuring Trainer...\n",
      "running on device cpu\n",
      "Trainer Configured\n"
     ]
    }
   ],
   "source": [
    "set_seed(424242)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "print(\"===== STARTING SUPERVISED FINE-TUNING =====\")\n",
    "\n",
    "# For Logging\n",
    "train_idx = []\n",
    "train_losses = []\n",
    "val_idx = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "valid_iters = 32\n",
    "block_size = 256\n",
    "train_ds = SFTSummarize(block_size=block_size, split='train')\n",
    "valid_ds = SFTSummarize(block_size=block_size, split='valid')\n",
    "prompt_ds = SummarizePrompt(block_size=block_size, split='valid')\n",
    "print(\"Data Loaded\")\n",
    "\n",
    "print(\"Creating Model...\")\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2'\n",
    "model_config.vocab_size = train_ds.get_vocab_size()\n",
    "model_config.block_size = block_size\n",
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "print(\"Model Created\")\n",
    "\n",
    "print(\"Configuring Trainer...\")\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-6\n",
    "train_config.num_workers = 2\n",
    "train_config.log_every = 50\n",
    "train_config.generate_every = 200\n",
    "train_config.save_every = None\n",
    "train_config.epochs = 1\n",
    "train_config.batch_size = 4\n",
    "train_config.compile = True\n",
    "trainer = Trainer(train_config, model, train_ds)\n",
    "print(\"Trainer Configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us know train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4be39666df0a4e6792ee199402a34d48",
      "b269a7cfaa3746f998fdb21b992612e4",
      "c272eae0659b409f8a1135756cec18a5",
      "84cafb7e56ae46e0886db53ca7b5c5c1",
      "0b0194a1df7241048b33cf980fb8aa28",
      "1336f8e8e8d041068b71148b86825f11",
      "51ff88274a854da2bfa98bf1a08a22d3",
      "c02dad90d14e45c48ccd3dd33185cd39",
      "b06f1079746148b995fba17fe1df0b7d",
      "ed33cb9f953c49e2b7a8b3f20a018683",
      "89f5c704d827440fb3db8deca612e161",
      "ad0cd0869b4c4efc80d266818ea5cdc5",
      "42dcf6ab5a594d619cec1988833d84e6",
      "248cb095229d4678bab99ec4aeaf1bb5",
      "5bb4fd032a434c1e976dbffd3da74b87",
      "154dd9543794443cb121ca82035261fe",
      "2919cb771a284958962745bc5e54c326",
      "a66c3e2f006c4331ae5a5f4344eeb51a",
      "5d28ed0616764255a2fb0e40f8cc8e77",
      "16f7553d9e8d446b87d10a2cc17be5ba",
      "9968c0728aff41d88e346a63e42d3a38",
      "d77845b84d09413daded97afb108d463",
      "1b7aeb3adaa1482084f9f8bdc3d16663",
      "8710e987705c47c2b5cf626b2ce900cd",
      "288e4872020a4a759604329e6daf06a7",
      "1b5b5db7d34248aa8f6867f2942f1391",
      "d3a5cc4453144371896c05680407bef7",
      "bd953611e5e5404c9c28592e04c1251a",
      "7c29ea003e454dc099e77d2bc4d6d974",
      "28ae83183ca6499f8d076ff9ca6b5ff5",
      "e10dc74231d0478eaff457080c535113",
      "c82e1f7950ee4d978ba32ef6b26efe74",
      "601e7cb98e5c44df904282a03b5a7ad1",
      "ac320594088b40319a2a2ae294635c5f",
      "46a84b519b624db396bfb59285815919",
      "85b3cbf533a1462cbaff18d1960b2201",
      "af7e73c2b750435b91d5a6b23b6838c1",
      "2a6a00d5f5b14633b3f43bebe7e6026c",
      "228c88b6ccb44207ade2f96b19e75608",
      "260fdb9b044a4773b8d6eaa1bfa5605e",
      "e5790596bfbf40acb8587d29d6f87731",
      "bb33d3518607482fb874e6643033ec53",
      "6d2fbeb3851f4a499f818b06510d27b3",
      "efbbb00b479d4e789ec8967f60f1e7b2",
      "c93e835cba4e40a597d6efcb4e496730",
      "d11d016ef8604648ae6dcc519693908a",
      "2eb7cf44c2ef497b87730f51e2736e1a",
      "ff9f8ceaa2b643169634d6e001caa0fb",
      "666e173915ce4991874d8fab374d9ae9",
      "2d9473f5c47741f7b1105d693ad86ea4",
      "29dd1787a7d341fc80c72e0c27da6bd0",
      "4b93bd0d4f714bb195b7e22518a4b979",
      "49e77d7ec5cb4586a74202f7577bc622",
      "9fac3a37e52d49a3826c6629657c77c5",
      "bd79e68f74dc4d38922fc2d18a4650c4",
      "be2dcb18de83433aaccb2b39a482ec44",
      "b76a95f213354fbba082e8f944f06e17",
      "c8a2c2a6dd044a8d8479b2444e939007",
      "1b28d7eb0d62443ab28617d5e1d0813a",
      "4cc7f1eda6864cedbe7349560a2ab813",
      "6de6243914f3414d9d5a1cf7c55530f1",
      "75719703ea164561b17d7add8a064df3",
      "ccb8dceeb5044822bbe86890c718de90",
      "3b2a880c4dbe4201b4374a8f99d1fb57",
      "b26d960e564544f6ad2f9616c53cc906",
      "f7d6a017c827451d8974475c357362d7",
      "b970498a12194dba978e9cfb6cb755f7",
      "2ac0bcbbaafe426b9436baa037a51bf4",
      "1ba6830843e44387bfaadd0d7dc075c0",
      "cd460bdad54d4bec8ff7c086d75e409c",
      "34367f6d5a8a412b8352feaa0e2db03b",
      "a43a140686364a689a5771cf006a01eb",
      "12df88cc9bbe416cbf46c98eef799892",
      "b49a02d2bcc442aa9b67442d3dae57b7",
      "e2f8361fa8f841efaaac0fd3eb4a3e33",
      "b89f250703b64cf1b8a1f7fa1f86e6ba",
      "98aa6a1a047144ad8645e7b9751a0ce0"
     ]
    },
    "id": "nwu6aK0UFsFF",
    "outputId": "3a3520c2-dff6-459e-b9f8-a5a44bfa11b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Validation loader created\n",
      "Setting up Callbacks\n",
      "Callbacks set\n",
      "Training...\n",
      "E: 0, iter_dt 0.00ms; iter 0: train loss 3.65614, val loss: 3.45290\n",
      "Generation 0: SUBREDDIT: r/relationships\n",
      "TITLE: When should I [M24] offer to start paying for things at my girlfriends [F26] place? Or at all?\n",
      "POST: We've been together officially for a little over a month now, but have been dating for closer to four months. I've known her almost three years now.\n",
      "\n",
      "Since things became official I've been spending nearly all my time at her place. She gave me a key and has said that it's half my home too. So my dog and I are there now all the time. I still have my own apartment (six months left on the lease). We've talked some about me moving in, which will happen officially once my lease is up. But if I'm spending all my time at her place, using heat, water, electricity, etc... Shouldn't I help pay for something? Or is it too soon to talk about that kind of thing?\n",
      "\n",
      "Her internet is very slow DSL and she's off contract.. I've thought about offering to have my much faster cable internet moved to her place and just keep paying it myself.. Thoughts?\n",
      "TL;DR: !!!\n",
      "\n",
      "UPDATE: I've actually made it into the next update. It's been a long, stressful week and a full-time job since I posted my post.\n",
      "\n",
      "As usual, I have a lot of great advice for my readers as well. Thank you all for your support. We've really had a really good go-back on all of you.\n",
      "\n",
      "Thanks for reading! See you on your honeymoon and happy new year!<|endoftext|>\n",
      "===== DONE SUPERVISED FINE-TUNING =====\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "device = trainer.device\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    batch_size=trainer.config.batch_size * 2,\n",
    ")\n",
    "print('Validation loader created')\n",
    "print('Setting up Callbacks')\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "print('Callbacks set')\n",
    "print(\"Training...\")\n",
    "trainer.run()\n",
    "\n",
    "print(\"===== DONE SUPERVISED FINE-TUNING =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_62355/2835103069.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"summarize_sft.pt\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"summarize_sft.pt\")\n",
    "\n",
    "# Load the model trained for a whole epoch\n",
    "model.load_state_dict(torch.load(\"summarize_sft.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a couple of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: SUBREDDIT: r/relationships\n",
      "TITLE: When should I [M24] offer to start paying for things at my girlfriends [F26] place? Or at all?\n",
      "POST: We've been together officially for a little over a month now, but have been dating for closer to four months. I've known her almost three years now.\n",
      "\n",
      "Since things became official I've been spending nearly all my time at her place. She gave me a key and has said that it's half my home too. So my dog and I are there now all the time. I still have my own apartment (six months left on the lease). We've talked some about me moving in, which will happen officially once my lease is up. But if I'm spending all my time at her place, using heat, water, electricity, etc... Shouldn't I help pay for something? Or is it too soon to talk about that kind of thing?\n",
      "\n",
      "Her internet is very slow DSL and she's off contract.. I've thought about offering to have my much faster cable internet moved to her place and just keep paying it myself.. Thoughts?\n",
      "TL;DR: Bored on my girlfriend's internet for a really long time, she said she's in the middle of a breakup. Is that okay for her?<|endoftext|>\n",
      "Generation 0: SUBREDDIT: r/relationships\n",
      "TITLE: Is there a non-creepy way for me [26M] to approach her [24F] at work?\n",
      "POST: Hi there, I'll try to keep it short.\n",
      "\n",
      "There is a girl who works at my go-to grocery store who I would like to get to know better. She really brighten's up my day when I go there and I'm absolutley stunned by her everytime I go there. I know it's hard to approach people at work, but the last thing I will do is creep outside the store until she heads home - HELL NO.\n",
      "\n",
      "I've been thinking of a way to approach her but everything seems kind of creepy, like just going up to her out of the blue and telling her that I think she's cute and that I would like to get to know her better. Is there a smoother way of doing this?\n",
      "\n",
      "I know nothing about her except for her Name and her Age. But something tells me that I would like to get to know her more. I've never done nor felt the urge to approach somebody outside of a bar.\n",
      "\n",
      "Thanks!\n",
      "TL;DR: \n",
      "Hi, I'm a 23F man who needs to know that I respect her better than someone else and that I want to get to know her better.\n",
      "TL:DR: Fam, I really didn't know what I was getting into.<|endoftext|>\n",
      "Generation 0: SUBREDDIT: r/AskReddit\n",
      "TITLE: Has screen-sharing been throttled on Skype and are there alternatives?\n",
      "POST: Hello friends!\n",
      "\n",
      "I am in a long distance relationship and as such Skype has been a major boon to the quality of our lives. This has been downgraded somewhat recently.\n",
      "\n",
      "Me and my girlfriend would screen-share so we could enjoy movies and episodes together. It might not seem like much but we cherished the ability. I subscribed to the Skype service solely to keep that for us. \n",
      "\n",
      "Some time ago screen-sharing just stopped being able to cope with running video. I think this was around the time Microsoft acquired Skype although I'm not sure. This is confounding since both of us still enjoy good web connections and use high definition webcams that stream very well under normal conditions. \n",
      "\n",
      "Furthermore we have discovered that if we screen-share for any reason our call is seemingly throttled and we have to recall to regain quality.\n",
      "\n",
      "I have googled this to no avail and I was wondering if anybody has any knowledge about this situation and/or knows a working alternative?\n",
      "TL;DR: Hey, I just received my first Skype for free today, will need to pay for any additional cost after my 2 year of waiting. I have no idea what to do?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i in [17, 42, 75]:\n",
    "    # Get a validation prompt to test\n",
    "    sample_prompt = prompt_ds[i].to(device)\n",
    "    idx = model.generate(sample_prompt, max_new_tokens=128, do_sample=True, top_k=30, stop_at=train_ds.tokenizer.eot_token).cpu()\n",
    "    for j,generation in enumerate(idx):\n",
    "        print(f\"Generation {j}:\", train_ds.tokenizer.decode(generation))\n",
    "\n",
    "# # Plot the losses\n",
    "# trainer.logger.plot({\"Loss\": [\"Train\", \"Valid\"]}, filename=\"summarize_sft.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"./images/summarize_sft_1ep_256.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8M4f-ucFsFG"
   },
   "source": [
    "## Step 2: Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"./images/RM_step2.png\" width=300px></center>\n",
    "<center width=\"100%\"><small>Step 2: Train a reward model based on human preferences</small><br><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znzfQoBlFsFG"
   },
   "source": [
    "**What do we mean by reward?** \n",
    "\n",
    "In the context of RL, the reward is the immediate feedback received from the environment after an action is taken.  It indicates the immediate benefit or cost associated with that action, helping the agent to learn which actions lead to favorable outcomes. For our application, the reward should reflect the quality of our summary. For example, a short summary correctly capturing the meaning of the original post should get a much higher reward from a summary that seems irrelevant to the original text.   \n",
    "\n",
    "**What data do need to train the RM?**\n",
    "\n",
    "The training dataset for our RM model should include prompt-generation pairs (i.e., post-summary) as well as capture somehow the preference of human users on the generation (summary). We can achieve this through the following process. Each prompt is passed through the initial language model to generate new text (summaries) multiple times. Then human annotators rank the generated text, for example for two generations, the annotator may indicate which one they prefer over the other. Finally, this preference may be added as a 'label' in the summary and be included in the training dataset. For our task, we will use the dataset [CarperAI/openai_summarize_comparisons](https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons) available in hugginface. This dataset consists of Reddit posts, and for each post, includes two summaries, where one (positive) was preferred of the other (negative) by a human annotator. Our goal is to train our RM model to 'prefer' the summaries, that our human annotators would also prefer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build and train our reward model! We start by defining some helper functions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w7iJ_mo7FsFH"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, config, ds, iters=32):\n",
    "    train_loader = DataLoader(\n",
    "        ds,\n",
    "        shuffle=False,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        drop_last=True\n",
    "    )\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    i = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss, acc = model(\n",
    "            batch[\"neg_toks\"],\n",
    "            attn_mask=batch[\"neg_mask\"],\n",
    "            positive_tokens=batch[\"pos_toks\"],\n",
    "            positive_mask=batch[\"pos_mask\"]\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        i += 1\n",
    "        if i == iters:\n",
    "            break\n",
    "    return total_loss / i, total_acc / i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2zhJ60sOK7GW"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def set_reward_bias(model, config, ds, iters=128, device='cpu'):\n",
    "    train_loader = DataLoader(\n",
    "        ds,\n",
    "        shuffle=False,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        drop_last=True\n",
    "    )\n",
    "    all_rewards = []\n",
    "    i = 0\n",
    "    for batch in train_loader:\n",
    "        x = torch.cat((batch[\"pos_toks\"], batch[\"neg_toks\"]))\n",
    "        mask = torch.cat((batch[\"pos_mask\"], batch[\"neg_mask\"]))\n",
    "        x, mask = [v.to(device) for v in (x, mask)]\n",
    "        model.to(device)\n",
    "        rewards = model(x, attn_mask=mask)\n",
    "        all_rewards.append(rewards)\n",
    "        i += 1\n",
    "        if i == iters:\n",
    "            break\n",
    "\n",
    "    reward_bias = torch.mean(torch.cat(all_rewards))\n",
    "    model.prediction_head.bias.sub_(reward_bias)\n",
    "    print(\"Set reward bias to\", model.prediction_head.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now configure the training data and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "train dataset: 13856 val dataset: 4373\n"
     ]
    }
   ],
   "source": [
    "set_seed(424242)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# simple supervised training loop for the reward model!\n",
    "\n",
    "# ------\n",
    "# Config\n",
    "# ------\n",
    "config = CN()\n",
    "\n",
    "config.num_workers = 2\n",
    "config.batch_size = 16\n",
    "config.learning_rate = 3e-6\n",
    "config.betas = (0.9, 0.95)\n",
    "config.weight_decay = 0.1\n",
    "config.grad_norm_clip = 1.0\n",
    "\n",
    "epochs = 3\n",
    "data_block_size = 256\n",
    "train_dataset = RewardModelSummarize(block_size=data_block_size, split='train')\n",
    "valid_dataset = RewardModelSummarize(block_size=data_block_size, split='valid1')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device\", device)\n",
    "\n",
    "print(\"train dataset:\", len(train_dataset), \"val dataset:\", len(valid_dataset))\n",
    "\n",
    "model_block_size = 1024\n",
    "config.model = GPT.get_default_config()\n",
    "config.model.model_type = \"gpt2\"\n",
    "config.model.n_layer = 12\n",
    "config.model.n_head = 12\n",
    "config.model.n_embd = 768\n",
    "config.model.resid_pdrop = 0\n",
    "config.model.attn_pdrop = 0\n",
    "config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "config.model.block_size = model_block_size\n",
    "\n",
    "# setup for logging\n",
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_996/570506187.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(\"trained_models/summarize_sft.pt\", map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: _IncompatibleKeys(missing_keys=['prediction_head.weight', 'prediction_head.bias'], unexpected_keys=['lm_head.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): RewardModel(\n",
       "    (transformer): Transformer(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (embd_drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): ModuleDict(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELU()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (prediction_head): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RewardModel(config.model)\n",
    "\n",
    "# Load model from finetuned model\n",
    "sd = torch.load(\"trained_models/summarize_sft.pt\", map_location=torch.device('cpu'))\n",
    "missing = model.load_state_dict(sd, strict=False)\n",
    "# We expect the lm head to be replaced by the scalar reward prediction head\n",
    "print(\"Missing keys:\", missing)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "uncompiled_model = model\n",
    "model = torch.compile(model)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = GPT.configure_optimizers(model, config)\n",
    "\n",
    "# -----\n",
    "# Train\n",
    "# -----\n",
    "# to speed up training, we can start by only training the reward head of the model\n",
    "# model.transformer.requires_grad_(False)\n",
    "# run(model, config, logger)\n",
    "\n",
    "config.batch_size = 8\n",
    "model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 0, Iter: 0, Train loss: 0.6301, Train Acc: 0.50, Grad norm: 1.4482, Val loss: 0.6929, Val Acc: 0.60 Took: 12s\n"
     ]
    }
   ],
   "source": [
    "# setup the dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "iter_num = 0\n",
    "iter_time = time.time()\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward the model\n",
    "        with try_auto_cast(device):\n",
    "            loss, acc = model(\n",
    "                batch[\"neg_toks\"],\n",
    "                attn_mask=batch[\"neg_mask\"],\n",
    "                positive_tokens=batch[\"pos_toks\"],\n",
    "                positive_mask=batch[\"pos_mask\"]\n",
    "            )\n",
    "\n",
    "        # backprop and update the parameters\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        if i % 100 == 0:\n",
    "            model.eval()\n",
    "            tnow = time.time()\n",
    "            iter_dt = tnow - iter_time\n",
    "            val_loss, val_acc = evaluate(model, config, valid_dataset)\n",
    "            print(f\"E: {epoch}, Iter: {iter_num}, Train loss: {loss.item():.4f}, Train Acc: {acc.item():.2f}, Grad norm: {grad_norm:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.2f} Took: {iter_dt:.0f}s\")\n",
    "            iter_time = tnow\n",
    "            model.train()\n",
    "\n",
    "            # Collect data for plotting\n",
    "            logger.log(\"Val Loss\", iter_num, val_loss)\n",
    "            logger.log(\"Val Acc\", iter_num, val_acc)\n",
    "\n",
    "        logger.log(\"Train Loss\", iter_num, loss.item())\n",
    "        logger.log(\"Train Acc\", iter_num, acc.item())\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == 1:\n",
    "            break\n",
    "# subtract the mean reward from the reward head to make it unbiased\n",
    "# set_reward_bias(model, config, train_dataset, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 0, Iter: 600, Train loss: 0.4050, Train Acc: 0.88, Grad norm: 10.8846, Val loss: 0.6921, Val Acc: 0.58 Took: 742s\n",
      "E: 0, Iter: 700, Train loss: 0.8046, Train Acc: 0.50, Grad norm: 10.0794, Val loss: 0.7137, Val Acc: 0.58 Took: 1001s\n",
      "E: 0, Iter: 800, Train loss: 0.5157, Train Acc: 0.88, Grad norm: 5.5163, Val loss: 0.6936, Val Acc: 0.60 Took: 983s\n",
      "E: 0, Iter: 900, Train loss: 0.4445, Train Acc: 0.88, Grad norm: 8.0555, Val loss: 0.7058, Val Acc: 0.59 Took: 999s\n",
      "E: 0, Iter: 1000, Train loss: 0.9042, Train Acc: 0.50, Grad norm: 21.1563, Val loss: 0.6852, Val Acc: 0.59 Took: 1000s\n",
      "E: 0, Iter: 1100, Train loss: 0.4658, Train Acc: 0.88, Grad norm: 7.8752, Val loss: 0.6933, Val Acc: 0.59 Took: 952s\n",
      "E: 0, Iter: 1200, Train loss: 0.7349, Train Acc: 0.50, Grad norm: 8.2510, Val loss: 0.7201, Val Acc: 0.60 Took: 993s\n",
      "E: 0, Iter: 1300, Train loss: 0.6682, Train Acc: 0.75, Grad norm: 6.9098, Val loss: 0.7029, Val Acc: 0.59 Took: 976s\n",
      "E: 0, Iter: 1400, Train loss: 0.6103, Train Acc: 0.75, Grad norm: 6.1839, Val loss: 0.6948, Val Acc: 0.60 Took: 995s\n",
      "E: 0, Iter: 1500, Train loss: 0.4223, Train Acc: 0.88, Grad norm: 7.2289, Val loss: 0.6801, Val Acc: 0.61 Took: 968s\n",
      "E: 0, Iter: 1600, Train loss: 0.4161, Train Acc: 0.88, Grad norm: 8.6824, Val loss: 0.6939, Val Acc: 0.60 Took: 32360s\n",
      "E: 0, Iter: 1700, Train loss: 0.7237, Train Acc: 0.38, Grad norm: 7.3021, Val loss: 0.6918, Val Acc: 0.60 Took: 563s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 10:00:17.750000 33714 torch/_dynamo/convert_frame.py:844] [0/20] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0112 10:00:17.750000 33714 torch/_dynamo/convert_frame.py:844] [0/20]    function: 'forward' (/Users/mbk-21-0452/Documents/TUK/NLP_TA/notebooks/tutorial_RLHF/mingpt/rewards.py:112)\n",
      "W0112 10:00:17.750000 33714 torch/_dynamo/convert_frame.py:844] [0/20]    last reason: 0/10: GLOBAL_STATE changed: grad_mode \n",
      "W0112 10:00:17.750000 33714 torch/_dynamo/convert_frame.py:844] [0/20] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0112 10:00:17.750000 33714 torch/_dynamo/convert_frame.py:844] [0/20] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set reward bias to -0.03496289998292923\n"
     ]
    }
   ],
   "source": [
    "# setup the dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "iter_num = 0\n",
    "iter_time = time.time()\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward the model\n",
    "        with try_auto_cast(device):\n",
    "            loss, acc = model(\n",
    "                batch[\"neg_toks\"],\n",
    "                attn_mask=batch[\"neg_mask\"],\n",
    "                positive_tokens=batch[\"pos_toks\"],\n",
    "                positive_mask=batch[\"pos_mask\"]\n",
    "            )\n",
    "\n",
    "        # backprop and update the parameters\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        if i % 100 == 0:\n",
    "            model.eval()\n",
    "            tnow = time.time()\n",
    "            iter_dt = tnow - iter_time\n",
    "            val_loss, val_acc = evaluate(model, config, valid_dataset)\n",
    "            print(f\"E: {epoch}, Iter: {iter_num}, Train loss: {loss.item():.4f}, Train Acc: {acc.item():.2f}, Grad norm: {grad_norm:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.2f} Took: {iter_dt:.0f}s\")\n",
    "            iter_time = tnow\n",
    "            model.train()\n",
    "\n",
    "            # Collect data for plotting\n",
    "            logger.log(\"Val Loss\", iter_num, val_loss)\n",
    "            logger.log(\"Val Acc\", iter_num, val_acc)\n",
    "\n",
    "        logger.log(\"Train Loss\", iter_num, loss.item())\n",
    "        logger.log(\"Train Acc\", iter_num, acc.item())\n",
    "\n",
    "        iter_num += 1\n",
    "# subtract the mean reward from the reward head to make it unbiased\n",
    "set_reward_bias(model, config, train_dataset, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save our model and draw some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(uncompiled_model.state_dict(), \"reward_model.pt\")\n",
    "logger.plot({\"Loss\": [\"Train Loss\", \"Val Loss\"], \"Accuracy\": [\"Train Acc\", \"Val Acc\"]}, filename=\"reward_model.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"reward_model.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpe1KSJBFsFI"
   },
   "source": [
    "## Step 3: Policy Optimization with PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oalF26-8FsFI"
   },
   "source": [
    "<center width=\"100%\"><img src=\"./images/RL_step3.png\" width=300px></center>\n",
    "<center width=\"100%\"><small>Step 3: Train the model with PPO</small><br><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRywIT3GFsFI"
   },
   "source": [
    "**How RL is relevant to our fine-tuning task?** \n",
    "\n",
    "We can consider our fine-tuning task as a RL problem, where: \n",
    "\n",
    "* the token sequence generated so far is the state\n",
    "* the next token to be sampled is the action \n",
    "* the next token distribution, predicted by our model, is the action policy\n",
    "* the reward predicted by our reward model is the environment reward\n",
    "\n",
    "*Note*: The reward may optionally be the predicted reward of the RM model minus a penalty term depending on the  Kullback–Leibler divergence (KL), shown as $D_{KL}$ below,  between the policy of the base model and the policy of the model during training. The KL divergence term penalizes the RL policy from moving substantially away from the initial model at each training step, which can be useful to make sure that the generated text is reasonable and coherent. Otherwise the model may start generating text, that may achieve high reward, but it is actually gibberish.\n",
    "\n",
    "We summarize below the RL training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"./images/RLHF_full.png\"></center>\n",
    "<center width=\"100%\"><small>The entire RL training process</small><br><br></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a helper function (runs on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "R_EXgL8LFsFJ"
   },
   "outputs": [],
   "source": [
    "def validate(valid_ds, model, reward_model, device, max_iters=64):\n",
    "    reward_model.eval()\n",
    "    model.eval()\n",
    "\n",
    "    total_rewards = 0\n",
    "    count = 0\n",
    "    total = min(max_iters, len(valid_ds))\n",
    "    valid_progress_bar = trange(total, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in valid_progress_bar:\n",
    "            prompt = valid_ds[i].to(device)\n",
    "\n",
    "            completion = model.generate(prompt, max_new_tokens=completion_len, do_sample=True, top_k=30, stop_at=end_of_text)\n",
    "\n",
    "            reward = reward_model(completion).item()\n",
    "            total_rewards += reward\n",
    "            count += 1\n",
    "            valid_progress_bar.set_postfix(avg_reward=f\"{total_rewards/count:.4f}\")\n",
    "\n",
    "            if i < 3:\n",
    "                print(train_ds.tokenizer.decode(completion[0]), f\"\\nReward: {reward}\\n========\\n\")\n",
    "\n",
    "    average_reward = total_rewards / total\n",
    "    model.train()\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define all the models we are going to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_996/4116362148.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"trained_models/summarize_sft.pt\", map_location='cpu'))\n",
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_996/4116362148.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reward_model.load_state_dict(torch.load(\"trained_models/reward_model.pt\", map_location='cpu'))\n",
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_996/4116362148.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_model.load_state_dict(torch.load(\"trained_models/reward_model.pt\", map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (transformer): Transformer(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (embd_drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (prediction_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(424242)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Transformer context length\n",
    "block_size = 1024\n",
    "\n",
    "# completion + prompt <= block_size\n",
    "completion_len = 80\n",
    "max_prompt_len = 256\n",
    "\n",
    "model = GPT.get_default_config()\n",
    "model.model_type = \"gpt2\"\n",
    "model.n_layer = 12\n",
    "model.n_head = 12\n",
    "model.n_embd = 768\n",
    "model.vocab_size = 50257\n",
    "model.model_type = None\n",
    "model.block_size = block_size\n",
    "\n",
    "reward_model = RewardModel(model)\n",
    "value_model = ValueModel(model)\n",
    "model = GPT(model)\n",
    "\n",
    "# Load reward, value, and model from weights!\n",
    "model.load_state_dict(torch.load(\"trained_models/summarize_sft.pt\", map_location='cpu'))\n",
    "reward_model.load_state_dict(torch.load(\"trained_models/reward_model.pt\", map_location='cpu'))\n",
    "value_model.load_state_dict(torch.load(\"trained_models/reward_model.pt\", map_location='cpu'))\n",
    "\n",
    "# reference model is the finetuned SFT model\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.requires_grad_(False)\n",
    "ref_model.eval()\n",
    "\n",
    "reward_model.requires_grad_(False)\n",
    "reward_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the models to GPU and compile them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "value_model.to(device)\n",
    "reward_model.to(device)\n",
    "ref_model.to(device)\n",
    "print(\"Running on device\", device)\n",
    "\n",
    "uncompiled_model = model\n",
    "uncompiled_value_model = value_model\n",
    "\n",
    "# compile the model\n",
    "model = torch.compile(model)\n",
    "reward_model = torch.compile(reward_model)\n",
    "value_model = torch.compile(value_model)\n",
    "ref_model = torch.compile(ref_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the training data and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds: 25130 val ds: 1409\n"
     ]
    }
   ],
   "source": [
    "# PPO hyperparams\n",
    "sample_batch_size = 4 # Number of completions to sample\n",
    "train_batch_size = 2 # OAI uses equal training and sampling batches of 64 (we'll use whatever fits on the GPU!)\n",
    "grad_accum_steps = sample_batch_size // train_batch_size\n",
    "max_learning_rate = 3e-6\n",
    "grad_norm_clip = 1.0\n",
    "kl_beta = 0.02\n",
    "n_updates = 2\n",
    "gamma = 1\n",
    "lambd = 0.95\n",
    "n_epochs = 1\n",
    "\n",
    "# Logging\n",
    "logger = Logger()\n",
    "\n",
    "# Prompt datasets\n",
    "train_ds = SummarizePrompt('train', block_size=max_prompt_len)\n",
    "valid_ds = SummarizePrompt('valid', block_size=max_prompt_len)\n",
    "end_of_text = train_ds.tokenizer.eot_token\n",
    "print(\"train ds:\", len(train_ds), \"val ds:\", len(valid_ds))\n",
    "\n",
    "# Can set separate lrs for policy and value fn\n",
    "total_iters = len(train_ds) // sample_batch_size * n_epochs\n",
    "get_lr = lr_schedule(max_learning_rate, max_iters=total_iters)\n",
    "optim_groups = [{'params': model.parameters()}, {'params': value_model.parameters()}]\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=get_lr(0), betas=(0.9, 0.95), fused=torch.cuda.is_available(), weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the training begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   2%|▏         | 1/64 [00:23<25:08, 23.94s/it, avg_reward=-0.7153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/AskReddit\n",
      "TITLE: How do you get someone out of your head?\n",
      "POST: Hi,\n",
      "I'm 22, and I have been with my girlfriend for 5 years now. We recently moved together. We've always loved each other intensely.\n",
      "\n",
      "Problem, I recently started to have feelings for an other person (a friend). This person has had a boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2 months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I'm not a good liar, and now she knows.\n",
      "\n",
      "We decided to give us a week alone, I went to my parents. \n",
      "\n",
      "Now, I'm completely lost. I keep on thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can't.  \n",
      "\n",
      "What do I do? It's been 3 months now, and I'm just desperate.\n",
      "TL;DR: I've come back from my depression, I have feelings for another person, and still need to convince someone.<|endoftext|> \n",
      "Reward: -0.7153141498565674\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 2/64 [00:32<15:35, 15.09s/it, avg_reward=0.0386] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/running\n",
      "TITLE: Knee pain due to poor balance\n",
      "POST: I've had difficulty with distance running due to strong knee pain. My endurance is great, I can cycle for very long distances, but I can't run because my knees give out around 8 to 10 mies.\n",
      "I went to the Orthopedist who did a full series of x-rays and pronounced my knees in excellent condition. Then he had me do a bunch of balance exercises and told me that balance and \"hip stability\" was my issue. He prescribed PT, but my insurance is kinda crappy and 3x's/week PT will run me around $300/month. That's a bit steep.\n",
      "So, has anyone else had knee issues due to balance and hip stability? What did you do? Are there balancing exercises I can do at home and not spend a ton of money on PT?\n",
      "TL;DR: A full series of x-rays in my knees, I'm not sure what I'm doing wrong. Should I go for balance or should I just go with a treadmill at home instead? Please help!<|endoftext|> \n",
      "Reward: 0.7925448417663574\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▍         | 3/64 [00:39<11:37, 11.44s/it, avg_reward=0.2231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/Pets\n",
      "TITLE: Pet lovers, how do you keep your home clean?\n",
      "POST: Everyone has their favorite tricks/tips to keeping a clean house, so I'm curious...and in the market for a new vacuum and/or steam mop. \n",
      "\n",
      "We have three adult cats and one Italian Greyhound puppy and live in a mostly hard-wood apartment [two carpeted rooms and two large area rugs]. The cats are short hair but shed like crazy [black, white and grey!] and IGs don't really shed at all, but track in a decent amount of dirt from the yard. Getting sick of sweeping, swiffering and then pushing around dirt with a mop. It'd be nice to have a vacuum that picks up dirt and hair effectively on hardwood and carpet and I'm strongly considering investing in a steam mop.\n",
      "\n",
      "So what do you do? What do you recommend?\n",
      "TL;DR: Got a cheap and effective vacuum and a steam muffin that cleans and mops everything in our living room, would be nice for an older cat or two.<|endoftext|> \n",
      "Reward: 0.5920672416687012\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (SFT) Val reward: -0.4824687163345516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:   0%|          | 0/6282 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/AskReddit\n",
      "TITLE: How do you get someone out of your head?\n",
      "POST: Hi,\n",
      "I'm 22, and I have been with my girlfriend for 5 years now. We recently moved together. We've always loved each other intensely.\n",
      "\n",
      "Problem, I recently started to have feelings for an other person (a friend). This person has had a boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2 months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I'm not a good liar, and now she knows.\n",
      "\n",
      "We decided to give us a week alone, I went to my parents. \n",
      "\n",
      "Now, I'm completely lost. I keep on thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can't.  \n",
      "\n",
      "What do I do? It's been 3 months now, and I'm just desperate.\n",
      "TL;DR: Girlfriend wants me to get out of my head, to leave her alone. I don't want to be in this situation. How do I get away?<|endoftext|> \n",
      "Reward: -0.6098144054412842\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/running\n",
      "TITLE: Knee pain due to poor balance\n",
      "POST: I've had difficulty with distance running due to strong knee pain. My endurance is great, I can cycle for very long distances, but I can't run because my knees give out around 8 to 10 mies.\n",
      "I went to the Orthopedist who did a full series of x-rays and pronounced my knees in excellent condition. Then he had me do a bunch of balance exercises and told me that balance and \"hip stability\" was my issue. He prescribed PT, but my insurance is kinda crappy and 3x's/week PT will run me around $300/month. That's a bit steep.\n",
      "So, has anyone else had knee issues due to balance and hip stability? What did you do? Are there balancing exercises I can do at home and not spend a ton of money on PT?\n",
      "TL;DR: Examined to see if it would be worth the money to spend on a back lift, ended up having knee issues. Went to Orthopedist, and he gave me a bunch of stress tests to get comfortable.<|endoftext|> \n",
      "Reward: 0.8704727292060852\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/Pets\n",
      "TITLE: Pet lovers, how do you keep your home clean?\n",
      "POST: Everyone has their favorite tricks/tips to keeping a clean house, so I'm curious...and in the market for a new vacuum and/or steam mop. \n",
      "\n",
      "We have three adult cats and one Italian Greyhound puppy and live in a mostly hard-wood apartment [two carpeted rooms and two large area rugs]. The cats are short hair but shed like crazy [black, white and grey!] and IGs don't really shed at all, but track in a decent amount of dirt from the yard. Getting sick of sweeping, swiffering and then pushing around dirt with a mop. It'd be nice to have a vacuum that picks up dirt and hair effectively on hardwood and carpet and I'm strongly considering investing in a steam mop.\n",
      "\n",
      "So what do you do? What do you recommend?\n",
      "TL;DR: Have a cat litter, and want to know what a good way to keep a nice new puppy clean. Advice posted here and here<|endoftext|> \n",
      "Reward: 0.40485984086990356\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Avg reward: -0.417, KL: 0.000, Value Loss: 1.9033, Grad Norm: 11.54, Vf grad norm: 18.26, Val reward: -0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:   0%|          | 0/6282 [11:27<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# PPO training loop:\n",
    "# 1. generate a set of completions, given some prompts for the task\n",
    "# 2. calculate the rewards, values and advantages of the completions\n",
    "# 3. optimize the models completions based on the rewards using ppo objective\n",
    "\n",
    "# initial reward on the validation set\n",
    "val_reward = validate(valid_ds, model, reward_model, device)\n",
    "print(\"Initial (SFT) Val reward:\", val_reward)\n",
    "\n",
    "i = 0\n",
    "for epoch in range(n_epochs):\n",
    "    batch_idxs = torch.randperm(len(train_ds))\n",
    "    for idx in trange(len(train_ds) // sample_batch_size, desc=\"iter\"):\n",
    "\n",
    "        # Learning rate schedule\n",
    "        curr_lr = get_lr(i)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = curr_lr\n",
    "\n",
    "        # Sample completions given some prompts from the dataset\n",
    "        # these are the `actions` in the RL sense that the model takes\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            value_model.eval()\n",
    "\n",
    "            original_log_probs = []\n",
    "            completions = []\n",
    "            advantages = []\n",
    "            returns = []\n",
    "            action_mask = []\n",
    "            targets = []\n",
    "            total_reward = 0\n",
    "            start_idx = idx * sample_batch_size\n",
    "            for prompt_idx in batch_idxs[start_idx : start_idx + sample_batch_size]:\n",
    "                prompt = train_ds[prompt_idx.item()].to(device)\n",
    "\n",
    "                # Sample the completions\n",
    "                completion = model.generate(prompt, max_new_tokens=completion_len, do_sample=True, top_k=30, stop_at=end_of_text)\n",
    "\n",
    "                if completion[0, -1] == end_of_text:\n",
    "                    # Evaluate and store the rewards for the last token\n",
    "                    reward = reward_model(completion).unsqueeze(-1)\n",
    "                else:\n",
    "                    # If there is no eot token, hardcode a negative reward\n",
    "                    reward = torch.tensor([[-1.0]], device=device)\n",
    "\n",
    "                total_reward += reward.item()\n",
    "                completion_minus_1, target = completion[:, :-1], completion[:, 1:]\n",
    "\n",
    "                # Store the model's original log prob (could be merged into the generate fn)\n",
    "                original_log_prob = model.log_probs(completion_minus_1, target)\n",
    "\n",
    "                # Reference logprobs\n",
    "                ref_log_prob = ref_model.log_probs(completion_minus_1, target)\n",
    "\n",
    "                # Calculate values, returns and advantages\n",
    "                values = value_model(completion)\n",
    "\n",
    "                # Calculate the advantage for our policy gradient\n",
    "                # Include the kl score to reduce overfitting\n",
    "                # the kl reward here could be kept up to date with the policy network\n",
    "                # inside the ppo updates below for a better regularization effect\n",
    "                kl = original_log_prob - ref_log_prob\n",
    "                score = torch.cat((- kl_beta * kl, reward), dim=1)\n",
    "                advantage, single_return = calculate_advantage_and_returns(score, values, gamma=gamma, lambd=lambd)\n",
    "\n",
    "                # Pad the values up to block_size with zeros\n",
    "                pad = torch.zeros(1, block_size - advantage.size(1), device=advantage.device, dtype=advantage.dtype)\n",
    "                advantages.append(torch.cat((advantage, pad), dim=1))\n",
    "                returns.append(torch.cat((single_return, pad), dim=1))\n",
    "\n",
    "                # pad the log probs with 1 extra 0\n",
    "                pad_plus_1 = torch.zeros(1, block_size - original_log_prob.size(1), device=advantage.device, dtype=advantage.dtype)\n",
    "                original_log_probs.append(torch.cat((original_log_prob, pad_plus_1), dim=1))\n",
    "\n",
    "                # Pad the tokens with longs\n",
    "                pad = torch.zeros(1, block_size - completion.size(1), device=completion.device, dtype=completion.dtype)\n",
    "                completions.append(torch.cat((completion, pad), dim=1))\n",
    "                pad = torch.zeros(1, block_size - target.size(1), device=target.device, dtype=target.dtype)\n",
    "                targets.append(torch.cat((target, pad), dim=1))\n",
    "\n",
    "                # The action mask is only the generated part of the completion\n",
    "                mask = torch.zeros(1, block_size, device=advantage.device, dtype=advantage.dtype)\n",
    "                mask[:, prompt.size(1):completion.size(1)] = 1\n",
    "                action_mask.append(mask)\n",
    "\n",
    "        # Stack the values into a batch\n",
    "        advantages = torch.cat(advantages)\n",
    "        returns = torch.cat(returns)\n",
    "        completions = torch.cat(completions)\n",
    "        original_log_probs = torch.cat(original_log_probs)\n",
    "        action_mask = torch.cat(action_mask)\n",
    "        targets = torch.cat(targets)\n",
    "\n",
    "\n",
    "        # Do the PPO update on the batch of data several times\n",
    "        model.train()\n",
    "        value_model.train()\n",
    "        for _ in range(n_updates):\n",
    "            b_inds = torch.randperm(sample_batch_size)\n",
    "            for start in range(0, sample_batch_size, train_batch_size):\n",
    "                end = start + train_batch_size\n",
    "\n",
    "                # Grab the mini-batches\n",
    "                mb_inds = b_inds[start:end]\n",
    "                mb_completion = completions[mb_inds]\n",
    "                mb_target = targets[mb_inds]\n",
    "                mb_original_logps = original_log_probs[mb_inds]\n",
    "                mb_advantages = advantages[mb_inds]\n",
    "                mb_returns = returns[mb_inds]\n",
    "                mb_action_mask = action_mask[mb_inds]\n",
    "\n",
    "                \n",
    "                with try_auto_cast(device):\n",
    "                    # Forward pass through the latest model\n",
    "                    log_probs = model.log_probs(mb_completion, mb_target)\n",
    "\n",
    "                    # Policy loss\n",
    "                    pg_loss = policy_loss(log_probs, mb_original_logps, mb_advantages, mb_action_mask)\n",
    "\n",
    "                    # Value loss\n",
    "                    new_value = value_model(mb_completion)\n",
    "                    v_loss = value_loss(new_value, mb_returns, mb_action_mask)\n",
    "                    \n",
    "                    loss = pg_loss + 0.1 * v_loss\n",
    "                    loss = loss / grad_accum_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "            policy_grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "            value_grad_norm = torch.nn.utils.clip_grad_norm_(value_model.parameters(), grad_norm_clip)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            value_model.zero_grad()\n",
    "\n",
    "        avg_reward = total_reward / sample_batch_size\n",
    "        \n",
    "        logger.log(\"Reward\", i, avg_reward)\n",
    "        logger.log(\"Value Loss\", i, v_loss.item())\n",
    "        logger.log(\"KL\", i, kl.mean().item())\n",
    "        logger.log(\"Policy Grad Norm\", i, policy_grad_norm.item())\n",
    "        logger.log(\"Value Grad Norm\", i, value_grad_norm.item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            val_reward = validate(valid_ds, model, reward_model, device)\n",
    "            logger.log(\"Val Reward\", i, val_reward)\n",
    "            print(f\"Iter: {i}, Avg reward: {avg_reward:.3f}, KL: {kl.mean().item():.3f}, Value Loss: {v_loss.item():.4f}, Grad Norm: {policy_grad_norm:.2f}, Vf grad norm: {value_grad_norm:.2f}, Val reward: {val_reward:.3f}\")\n",
    "\n",
    "            torch.save(uncompiled_model.state_dict(), f\"trained_models/tmp_summarize_rl_{i}.pt\")\n",
    "            break\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our data and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(uncompiled_model.state_dict(), \"summarize_rl.pt\")\n",
    "torch.save(uncompiled_value_model.state_dict(), \"value_model.pt\")\n",
    "\n",
    "# Plot the results\n",
    "logger.plot({\"Reward\": [\"Reward\", \"Val Reward\"], \"Value Loss\": [\"Value Loss\"]}, filename=\"summarize_rl_rewards.png\")\n",
    "logger.plot({\"Gradient Norm\": [\"Policy Grad Norm\", \"Value Grad Norm\"], \"KL Div\": [\"KL\"], \"Clip Fraction\": [\"Clip Frac\"]}, filename=\"summarize_rl_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center width=\"100%\"><img src=\"./images/summarize_rl_rewards.png\"></center>\n",
    "<center width=\"100%\"><img src=\"./images/summarize_rl_metrics.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some summaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/kzvk79wn0nbdwg677rc3r7qr0000gp/T/ipykernel_996/3379542078.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"trained_models/summarize_rl.pt\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: SUBREDDIT: r/relationships\n",
      "TITLE: When should I [M24] offer to start paying for things at my girlfriends [F26] place? Or at all?\n",
      "POST: We've been together officially for a little over a month now, but have been dating for closer to four months. I've known her almost three years now.\n",
      "\n",
      "Since things became official I've been spending nearly all my time at her place. She gave me a key and has said that it's half my home too. So my dog and I are there now all the time. I still have my own apartment (six months left on the lease). We've talked some about me moving in, which will happen officially once my lease is up. But if I'm spending all my time at her place, using heat, water, electricity, etc... Shouldn't I help pay for something? Or is it too soon to talk about that kind of thing?\n",
      "\n",
      "Her internet is very slow DSL and she's off contract.. I've thought about offering to have my much faster cable internet moved to her place and just keep paying it myself.. Thoughts?\n",
      "TL;DR: When should I [M24] offer to start paying for things at my girlfriends [F26] place? Or at all? We've been together officially for a little over a month now, but have been dating for closer to four months.<|endoftext|>\n",
      "Generation 0: SUBREDDIT: r/relationships\n",
      "TITLE: Is there a non-creepy way for me [26M] to approach her [24F] at work?\n",
      "POST: Hi there, I'll try to keep it short.\n",
      "\n",
      "There is a girl who works at my go-to grocery store who I would like to get to know better. She really brighten's up my day when I go there and I'm absolutley stunned by her everytime I go there. I know it's hard to approach people at work, but the last thing I will do is creep outside the store until she heads home - HELL NO.\n",
      "\n",
      "I've been thinking of a way to approach her but everything seems kind of creepy, like just going up to her out of the blue and telling her that I think she's cute and that I would like to get to know her better. Is there a smoother way of doing this?\n",
      "\n",
      "I know nothing about her except for her Name and her Age. But something tells me that I would like to get to know her more. I've never done nor felt the urge to approach somebody outside of a bar.\n",
      "\n",
      "Thanks!\n",
      "TL;DR: Is there a non-creepy way for me [26M] to approach her [24F] at work? Hi there, I'll try to keep it short.<|endoftext|>\n",
      "Generation 0: SUBREDDIT: r/AskReddit\n",
      "TITLE: Has screen-sharing been throttled on Skype and are there alternatives?\n",
      "POST: Hello friends!\n",
      "\n",
      "I am in a long distance relationship and as such Skype has been a major boon to the quality of our lives. This has been downgraded somewhat recently.\n",
      "\n",
      "Me and my girlfriend would screen-share so we could enjoy movies and episodes together. It might not seem like much but we cherished the ability. I subscribed to the Skype service solely to keep that for us. \n",
      "\n",
      "Some time ago screen-sharing just stopped being able to cope with running video. I think this was around the time Microsoft acquired Skype although I'm not sure. This is confounding since both of us still enjoy good web connections and use high definition webcams that stream very well under normal conditions. \n",
      "\n",
      "Furthermore we have discovered that if we screen-share for any reason our call is seemingly throttled and we have to recall to regain quality.\n",
      "\n",
      "I have googled this to no avail and I was wondering if anybody has any knowledge about this situation and/or knows a working alternative?\n",
      "TL;DR: Has screen-sharing been throttled on Skype and are there alternatives? Hello friends! I am in a long distance relationship and as such Skype has been a major boon to the quality of our lives.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load the model trained for a whole epoch\n",
    "model.load_state_dict(torch.load(\"trained_models/summarize_rl.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "for i in [17, 42, 75]:\n",
    "    # Get a validation prompt to test\n",
    "    sample_prompt = prompt_ds[i].to(device)\n",
    "    idx = model.generate(sample_prompt, max_new_tokens=128, do_sample=True, top_k=30, stop_at=train_ds.tokenizer.eot_token).cpu()\n",
    "    for j,generation in enumerate(idx):\n",
    "        print(f\"Generation {j}:\", train_ds.tokenizer.decode(generation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   2%|▏         | 1/64 [00:09<09:59,  9.51s/it, avg_reward=1.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/AskReddit\n",
      "TITLE: How do you get someone out of your head?\n",
      "POST: Hi,\n",
      "I'm 22, and I have been with my girlfriend for 5 years now. We recently moved together. We've always loved each other intensely.\n",
      "\n",
      "Problem, I recently started to have feelings for an other person (a friend). This person has had a boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2 months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I'm not a good liar, and now she knows.\n",
      "\n",
      "We decided to give us a week alone, I went to my parents. \n",
      "\n",
      "Now, I'm completely lost. I keep on thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can't.  \n",
      "\n",
      "What do I do? It's been 3 months now, and I'm just desperate.\n",
      "TL;DR: How do you get someone out of your head? I'm 22, and I have been with my girlfriend for 5 years now. We recently moved together. We've always loved each other intensely.<|endoftext|> \n",
      "Reward: 1.0718774795532227\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 2/64 [00:20<10:27, 10.12s/it, avg_reward=1.7202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/running\n",
      "TITLE: Knee pain due to poor balance\n",
      "POST: I've had difficulty with distance running due to strong knee pain. My endurance is great, I can cycle for very long distances, but I can't run because my knees give out around 8 to 10 mies.\n",
      "I went to the Orthopedist who did a full series of x-rays and pronounced my knees in excellent condition. Then he had me do a bunch of balance exercises and told me that balance and \"hip stability\" was my issue. He prescribed PT, but my insurance is kinda crappy and 3x's/week PT will run me around $300/month. That's a bit steep.\n",
      "So, has anyone else had knee issues due to balance and hip stability? What did you do? Are there balancing exercises I can do at home and not spend a ton of money on PT?\n",
      "TL;DR: Knee pain due to poor balance I've had difficulty with distance running due to strong knee pain. My endurance is great, I can cycle for very long distances, but I can't run because my knees give out around 8 to 10 mies.<|endoftext|> \n",
      "Reward: 2.3685431480407715\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▍         | 3/64 [00:27<08:54,  8.77s/it, avg_reward=1.9577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/Pets\n",
      "TITLE: Pet lovers, how do you keep your home clean?\n",
      "POST: Everyone has their favorite tricks/tips to keeping a clean house, so I'm curious...and in the market for a new vacuum and/or steam mop. \n",
      "\n",
      "We have three adult cats and one Italian Greyhound puppy and live in a mostly hard-wood apartment [two carpeted rooms and two large area rugs]. The cats are short hair but shed like crazy [black, white and grey!] and IGs don't really shed at all, but track in a decent amount of dirt from the yard. Getting sick of sweeping, swiffering and then pushing around dirt with a mop. It'd be nice to have a vacuum that picks up dirt and hair effectively on hardwood and carpet and I'm strongly considering investing in a steam mop.\n",
      "\n",
      "So what do you do? What do you recommend?\n",
      "TL;DR: Everyone has their favorite tricks/tips to keeping a clean house, so I'm curious...and in the market for a new vacuum and/or steam mop.<|endoftext|> \n",
      "Reward: 2.432692289352417\n",
      "========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "val_reward = validate(valid_ds, model, reward_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reward after RLHF: 1.4084996758028865\n"
     ]
    }
   ],
   "source": [
    "print(\"Val Reward after RLHF:\", val_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  https://www.restack.io/p/reinforcement-learning-answer-reward-model-purpose-cat-ai\n",
    "*  https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning\n",
    "*  https://huggingface.co/blog/rlhf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b0194a1df7241048b33cf980fb8aa28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12df88cc9bbe416cbf46c98eef799892": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1336f8e8e8d041068b71148b86825f11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "154dd9543794443cb121ca82035261fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16f7553d9e8d446b87d10a2cc17be5ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b28d7eb0d62443ab28617d5e1d0813a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b26d960e564544f6ad2f9616c53cc906",
      "placeholder": "​",
      "style": "IPY_MODEL_f7d6a017c827451d8974475c357362d7",
      "value": " 548M/548M [00:05&lt;00:00, 162MB/s]"
     }
    },
    "1b5b5db7d34248aa8f6867f2942f1391": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c82e1f7950ee4d978ba32ef6b26efe74",
      "placeholder": "​",
      "style": "IPY_MODEL_601e7cb98e5c44df904282a03b5a7ad1",
      "value": " 6447/6447 [00:08&lt;00:00, 1020.59 examples/s]"
     }
    },
    "1b7aeb3adaa1482084f9f8bdc3d16663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8710e987705c47c2b5cf626b2ce900cd",
       "IPY_MODEL_288e4872020a4a759604329e6daf06a7",
       "IPY_MODEL_1b5b5db7d34248aa8f6867f2942f1391"
      ],
      "layout": "IPY_MODEL_d3a5cc4453144371896c05680407bef7"
     }
    },
    "1ba6830843e44387bfaadd0d7dc075c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b49a02d2bcc442aa9b67442d3dae57b7",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2f8361fa8f841efaaac0fd3eb4a3e33",
      "value": 124
     }
    },
    "228c88b6ccb44207ade2f96b19e75608": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248cb095229d4678bab99ec4aeaf1bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d28ed0616764255a2fb0e40f8cc8e77",
      "max": 6447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16f7553d9e8d446b87d10a2cc17be5ba",
      "value": 6447
     }
    },
    "260fdb9b044a4773b8d6eaa1bfa5605e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "288e4872020a4a759604329e6daf06a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28ae83183ca6499f8d076ff9ca6b5ff5",
      "max": 6447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e10dc74231d0478eaff457080c535113",
      "value": 6447
     }
    },
    "28ae83183ca6499f8d076ff9ca6b5ff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2919cb771a284958962745bc5e54c326": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29dd1787a7d341fc80c72e0c27da6bd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a6a00d5f5b14633b3f43bebe7e6026c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ac0bcbbaafe426b9436baa037a51bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43a140686364a689a5771cf006a01eb",
      "placeholder": "​",
      "style": "IPY_MODEL_12df88cc9bbe416cbf46c98eef799892",
      "value": "generation_config.json: 100%"
     }
    },
    "2d9473f5c47741f7b1105d693ad86ea4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eb7cf44c2ef497b87730f51e2736e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b93bd0d4f714bb195b7e22518a4b979",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49e77d7ec5cb4586a74202f7577bc622",
      "value": 665
     }
    },
    "34367f6d5a8a412b8352feaa0e2db03b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b2a880c4dbe4201b4374a8f99d1fb57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42dcf6ab5a594d619cec1988833d84e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2919cb771a284958962745bc5e54c326",
      "placeholder": "​",
      "style": "IPY_MODEL_a66c3e2f006c4331ae5a5f4344eeb51a",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "46a84b519b624db396bfb59285815919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_228c88b6ccb44207ade2f96b19e75608",
      "placeholder": "​",
      "style": "IPY_MODEL_260fdb9b044a4773b8d6eaa1bfa5605e",
      "value": ""
     }
    },
    "49e77d7ec5cb4586a74202f7577bc622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b93bd0d4f714bb195b7e22518a4b979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4be39666df0a4e6792ee199402a34d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b269a7cfaa3746f998fdb21b992612e4",
       "IPY_MODEL_c272eae0659b409f8a1135756cec18a5",
       "IPY_MODEL_84cafb7e56ae46e0886db53ca7b5c5c1"
      ],
      "layout": "IPY_MODEL_0b0194a1df7241048b33cf980fb8aa28"
     }
    },
    "4cc7f1eda6864cedbe7349560a2ab813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51ff88274a854da2bfa98bf1a08a22d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bb4fd032a434c1e976dbffd3da74b87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9968c0728aff41d88e346a63e42d3a38",
      "placeholder": "​",
      "style": "IPY_MODEL_d77845b84d09413daded97afb108d463",
      "value": " 6447/6447 [00:09&lt;00:00, 761.54 examples/s]"
     }
    },
    "5d28ed0616764255a2fb0e40f8cc8e77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "601e7cb98e5c44df904282a03b5a7ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "666e173915ce4991874d8fab374d9ae9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2fbeb3851f4a499f818b06510d27b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de6243914f3414d9d5a1cf7c55530f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75719703ea164561b17d7add8a064df3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c29ea003e454dc099e77d2bc4d6d974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84cafb7e56ae46e0886db53ca7b5c5c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed33cb9f953c49e2b7a8b3f20a018683",
      "placeholder": "​",
      "style": "IPY_MODEL_89f5c704d827440fb3db8deca612e161",
      "value": " 116722/116722 [02:08&lt;00:00, 845.66 examples/s]"
     }
    },
    "85b3cbf533a1462cbaff18d1960b2201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5790596bfbf40acb8587d29d6f87731",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb33d3518607482fb874e6643033ec53",
      "value": 0
     }
    },
    "8710e987705c47c2b5cf626b2ce900cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd953611e5e5404c9c28592e04c1251a",
      "placeholder": "​",
      "style": "IPY_MODEL_7c29ea003e454dc099e77d2bc4d6d974",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "89f5c704d827440fb3db8deca612e161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98aa6a1a047144ad8645e7b9751a0ce0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9968c0728aff41d88e346a63e42d3a38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fac3a37e52d49a3826c6629657c77c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43a140686364a689a5771cf006a01eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a66c3e2f006c4331ae5a5f4344eeb51a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac320594088b40319a2a2ae294635c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46a84b519b624db396bfb59285815919",
       "IPY_MODEL_85b3cbf533a1462cbaff18d1960b2201",
       "IPY_MODEL_af7e73c2b750435b91d5a6b23b6838c1"
      ],
      "layout": "IPY_MODEL_2a6a00d5f5b14633b3f43bebe7e6026c"
     }
    },
    "ad0cd0869b4c4efc80d266818ea5cdc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42dcf6ab5a594d619cec1988833d84e6",
       "IPY_MODEL_248cb095229d4678bab99ec4aeaf1bb5",
       "IPY_MODEL_5bb4fd032a434c1e976dbffd3da74b87"
      ],
      "layout": "IPY_MODEL_154dd9543794443cb121ca82035261fe"
     }
    },
    "af7e73c2b750435b91d5a6b23b6838c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d2fbeb3851f4a499f818b06510d27b3",
      "placeholder": "​",
      "style": "IPY_MODEL_efbbb00b479d4e789ec8967f60f1e7b2",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "b06f1079746148b995fba17fe1df0b7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b269a7cfaa3746f998fdb21b992612e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1336f8e8e8d041068b71148b86825f11",
      "placeholder": "​",
      "style": "IPY_MODEL_51ff88274a854da2bfa98bf1a08a22d3",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "b26d960e564544f6ad2f9616c53cc906": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b49a02d2bcc442aa9b67442d3dae57b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76a95f213354fbba082e8f944f06e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6de6243914f3414d9d5a1cf7c55530f1",
      "placeholder": "​",
      "style": "IPY_MODEL_75719703ea164561b17d7add8a064df3",
      "value": "model.safetensors: 100%"
     }
    },
    "b89f250703b64cf1b8a1f7fa1f86e6ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b970498a12194dba978e9cfb6cb755f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ac0bcbbaafe426b9436baa037a51bf4",
       "IPY_MODEL_1ba6830843e44387bfaadd0d7dc075c0",
       "IPY_MODEL_cd460bdad54d4bec8ff7c086d75e409c"
      ],
      "layout": "IPY_MODEL_34367f6d5a8a412b8352feaa0e2db03b"
     }
    },
    "bb33d3518607482fb874e6643033ec53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd79e68f74dc4d38922fc2d18a4650c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd953611e5e5404c9c28592e04c1251a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be2dcb18de83433aaccb2b39a482ec44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b76a95f213354fbba082e8f944f06e17",
       "IPY_MODEL_c8a2c2a6dd044a8d8479b2444e939007",
       "IPY_MODEL_1b28d7eb0d62443ab28617d5e1d0813a"
      ],
      "layout": "IPY_MODEL_4cc7f1eda6864cedbe7349560a2ab813"
     }
    },
    "c02dad90d14e45c48ccd3dd33185cd39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c272eae0659b409f8a1135756cec18a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c02dad90d14e45c48ccd3dd33185cd39",
      "max": 116722,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b06f1079746148b995fba17fe1df0b7d",
      "value": 116722
     }
    },
    "c82e1f7950ee4d978ba32ef6b26efe74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8a2c2a6dd044a8d8479b2444e939007": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccb8dceeb5044822bbe86890c718de90",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b2a880c4dbe4201b4374a8f99d1fb57",
      "value": 548105171
     }
    },
    "c93e835cba4e40a597d6efcb4e496730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d11d016ef8604648ae6dcc519693908a",
       "IPY_MODEL_2eb7cf44c2ef497b87730f51e2736e1a",
       "IPY_MODEL_ff9f8ceaa2b643169634d6e001caa0fb"
      ],
      "layout": "IPY_MODEL_666e173915ce4991874d8fab374d9ae9"
     }
    },
    "ccb8dceeb5044822bbe86890c718de90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd460bdad54d4bec8ff7c086d75e409c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b89f250703b64cf1b8a1f7fa1f86e6ba",
      "placeholder": "​",
      "style": "IPY_MODEL_98aa6a1a047144ad8645e7b9751a0ce0",
      "value": " 124/124 [00:00&lt;00:00, 7.90kB/s]"
     }
    },
    "d11d016ef8604648ae6dcc519693908a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d9473f5c47741f7b1105d693ad86ea4",
      "placeholder": "​",
      "style": "IPY_MODEL_29dd1787a7d341fc80c72e0c27da6bd0",
      "value": "config.json: 100%"
     }
    },
    "d3a5cc4453144371896c05680407bef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d77845b84d09413daded97afb108d463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e10dc74231d0478eaff457080c535113": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2f8361fa8f841efaaac0fd3eb4a3e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e5790596bfbf40acb8587d29d6f87731": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ed33cb9f953c49e2b7a8b3f20a018683": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efbbb00b479d4e789ec8967f60f1e7b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7d6a017c827451d8974475c357362d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff9f8ceaa2b643169634d6e001caa0fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fac3a37e52d49a3826c6629657c77c5",
      "placeholder": "​",
      "style": "IPY_MODEL_bd79e68f74dc4d38922fc2d18a4650c4",
      "value": " 665/665 [00:00&lt;00:00, 43.8kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
